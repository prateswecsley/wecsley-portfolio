<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NLP Topic Modeling - Project Details | Wecsley Prates</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; padding: 2rem 0; }
        .container { max-width: 1200px; margin: 0 auto; padding: 0 2rem; }
        .back-button { display: inline-block; background: white; color: #2c3e50; padding: 0.8rem 1.5rem; text-decoration: none; border-radius: 5px; margin-bottom: 2rem; font-weight: 600; transition: all 0.3s; box-shadow: 0 4px 15px rgba(0,0,0,0.1); }
        .back-button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(0,0,0,0.15); }
        .project-header { background: linear-gradient(135deg, #9b59b6 0%, #8e44ad 100%); color: white; padding: 3rem 2rem; border-radius: 15px; margin-bottom: 2rem; box-shadow: 0 10px 30px rgba(0,0,0,0.2); }
        .project-header h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        .project-header .company { font-size: 1.2rem; opacity: 0.9; margin-bottom: 1rem; }
        .project-header .date { font-size: 1rem; opacity: 0.8; }
        .content-section { background: white; padding: 2.5rem; border-radius: 15px; margin-bottom: 2rem; box-shadow: 0 5px 20px rgba(0,0,0,0.1); }
        .content-section h2 { color: #2c3e50; font-size: 1.8rem; margin-bottom: 1.5rem; padding-bottom: 0.5rem; border-bottom: 3px solid #9b59b6; }
        .content-section h3 { color: #34495e; font-size: 1.4rem; margin-top: 1.5rem; margin-bottom: 1rem; }
        .content-section h4 { color: #7f8c8d; font-size: 1.1rem; margin-top: 1rem; margin-bottom: 0.5rem; }
        .content-section p { margin-bottom: 1rem; text-align: justify; }
        .content-section ul, .content-section ol { margin-left: 2rem; margin-bottom: 1rem; }
        .content-section li { margin-bottom: 0.5rem; }
        .highlight-box { background: #f8f9fa; border-left: 4px solid #9b59b6; padding: 1.5rem; margin: 1.5rem 0; border-radius: 5px; }
        .tech-stack { display: flex; flex-wrap: wrap; gap: 0.8rem; margin: 1rem 0; }
        .tech-badge { background: linear-gradient(135deg, #9b59b6, #8e44ad); color: white; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem; font-weight: 600; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        .metrics-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1.5rem; margin: 2rem 0; }
        .metric-card { background: linear-gradient(135deg, #3498db, #2980b9); color: white; padding: 1.5rem; border-radius: 10px; text-align: center; box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
        .metric-number { font-size: 2.5rem; font-weight: bold; margin-bottom: 0.5rem; }
        .metric-label { font-size: 1rem; opacity: 0.9; }
        .code-block { background: #2c3e50; color: #ecf0f1; padding: 1.5rem; border-radius: 8px; overflow-x: auto; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem; line-height: 1.5; white-space: pre; }
        .data-flow { background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); padding: 2rem; border-radius: 10px; margin: 1.5rem 0; }
        .flow-step { display: flex; align-items: center; margin: 1rem 0; padding: 1rem; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.05); }
        .flow-step .step-number { background: #9b59b6; color: white; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem; flex-shrink: 0; }
        .impact-section { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 2.5rem; border-radius: 15px; margin-top: 2rem; }
        .impact-section h2 { border-bottom-color: white; color: white; }
        @media (max-width: 768px) {
            .project-header h1 { font-size: 2rem; }
            .content-section { padding: 1.5rem; }
            .metrics-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-button">‚Üê Back to Portfolio</a>

        <div class="project-header">
            <h1>üí¨ NLP Topic Modeling System</h1>
            <div class="company">LexisNexis Risk Solutions</div>
            <div class="date">January 2023 - June 2023 | Alpharetta, GA</div>
        </div>

        <div class="content-section">
            <h2>Project Overview</h2>
            <p>
                As a Data Scientist at LexisNexis, I built an advanced NLP-based Topic Modeling system to analyze thousands of monthly customer feedback submissions across multiple channels. Using Latent Dirichlet Allocation (LDA) and modern text preprocessing techniques, this system automatically identified emerging themes, pain points, and opportunities for product improvement.
            </p>
            <div class="highlight-box">
                <strong>Business Challenge:</strong> LexisNexis received 15,000+ customer feedback submissions monthly via surveys, support tickets, product reviews, and sales calls. The manual review process was time-consuming, inconsistent, and unable to identify systematic issues or trending topics in real-time.
            </div>
            <p>
                The solution delivered actionable insights through interactive Power BI and R-Shiny dashboards, enabling product teams and customer support to prioritize improvements based on data-driven topic analysis.
            </p>
        </div>

        <div class="content-section">
            <h2>Business Context & Problem Statement</h2>
            
            <h3>The Customer Feedback Challenge</h3>
            <ul>
                <li><strong>Data Volume:</strong> 15,000+ feedback items per month across 5 channels</li>
                <li><strong>Unstructured Text:</strong> Free-form responses, varying lengths (10-500 words)</li>
                <li><strong>Manual Analysis:</strong> Team of 8 analysts spending 160 hours/month on categorization</li>
                <li><strong>Delayed Insights:</strong> 3-4 week lag between feedback collection and actionable reports</li>
                <li><strong>Inconsistent Categorization:</strong> 30% disagreement rate between analysts on category assignment</li>
                <li><strong>Missed Patterns:</strong> No systematic way to identify emerging issues or trending topics</li>
            </ul>

            <h3>Feedback Channels Analyzed</h3>
            <div class="tech-stack">
                <span class="tech-badge">Customer Surveys (NPS)</span>
                <span class="tech-badge">Support Tickets</span>
                <span class="tech-badge">Product Reviews</span>
                <span class="tech-badge">Sales Call Transcripts</span>
                <span class="tech-badge">Social Media Mentions</span>
            </div>

            <h3>Key Business Objectives</h3>
            <ol>
                <li>Automate topic discovery from unstructured customer feedback</li>
                <li>Identify trending issues and emerging themes in real-time</li>
                <li>Improve first-call resolution by surfacing common pain points</li>
                <li>Enable product teams to prioritize features based on customer voice</li>
                <li>Track topic trends over time to measure improvement initiatives</li>
                <li>Create executive-ready dashboards with actionable insights</li>
            </ol>

            <h3>Success Metrics</h3>
            <div class="highlight-box">
                <strong>Target Goals:</strong>
                <ul style="margin-top: 0.5rem;">
                    <li>Reduce manual analysis time by 80%</li>
                    <li>Achieve 90%+ accuracy vs. human-labeled topics</li>
                    <li>Deliver insights within 24 hours of feedback submission</li>
                    <li>Improve first-call resolution by 10%+</li>
                </ul>
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 1: Data Collection & Integration</h2>

            <h3>1.1 Multi-Source Data Extraction</h3>
            <div class="code-block">
# Python script for feedback data extraction
import pandas as pd
from sqlalchemy import create_engine
import requests
from datetime import datetime, timedelta

def extract_feedback_data(start_date, end_date):
    """
    Extract customer feedback from multiple sources
    """
    
    # Database connection
    engine = create_engine('postgresql://user:pass@host:5432/feedback_db')
    
    # 1. Customer Surveys (NPS + open-ended responses)
    survey_query = """
    SELECT 
        feedback_id,
        customer_id,
        submission_date,
        nps_score,
        feedback_text,
        product_line,
        customer_segment,
        'survey' as source
    FROM customer_surveys
    WHERE submission_date BETWEEN %s AND %s
        AND feedback_text IS NOT NULL
        AND LENGTH(feedback_text) > 20
    """
    
    surveys_df = pd.read_sql(survey_query, engine, 
                             params=[start_date, end_date])
    
    # 2. Support Tickets (description + resolution notes)
    tickets_query = """
    SELECT 
        ticket_id as feedback_id,
        customer_id,
        created_date as submission_date,
        NULL as nps_score,
        CONCAT(ticket_description, ' ', resolution_notes) as feedback_text,
        product_line,
        customer_segment,
        'support_ticket' as source
    FROM support_tickets
    WHERE created_date BETWEEN %s AND %s
        AND ticket_description IS NOT NULL
    """
    
    tickets_df = pd.read_sql(tickets_query, engine,
                            params=[start_date, end_date])
    
    # 3. Product Reviews (from website and third-party platforms)
    reviews_query = """
    SELECT 
        review_id as feedback_id,
        customer_id,
        review_date as submission_date,
        star_rating as nps_score,
        review_text as feedback_text,
        product_line,
        customer_segment,
        'product_review' as source
    FROM product_reviews
    WHERE review_date BETWEEN %s AND %s
    """
    
    reviews_df = pd.read_sql(reviews_query, engine,
                            params=[start_date, end_date])
    
    # 4. Sales Call Transcripts (using speech-to-text API)
    # Already processed and stored
    calls_query = """
    SELECT 
        call_id as feedback_id,
        customer_id,
        call_date as submission_date,
        sentiment_score as nps_score,
        transcript_text as feedback_text,
        product_line,
        customer_segment,
        'sales_call' as source
    FROM call_transcripts
    WHERE call_date BETWEEN %s AND %s
        AND contains_feedback_flag = TRUE
    """
    
    calls_df = pd.read_sql(calls_query, engine,
                          params=[start_date, end_date])
    
    # Combine all sources
    all_feedback = pd.concat([surveys_df, tickets_df, reviews_df, calls_df],
                            ignore_index=True)
    
    print(f"Total feedback items: {len(all_feedback):,}")
    print(f"By source:\n{all_feedback['source'].value_counts()}")
    
    return all_feedback

# Extract 6 months of data for model training
feedback_df = extract_feedback_data('2023-01-01', '2023-06-30')

# Output:
# Total feedback items: 89,542
# By source:
# support_ticket    38,234
# survey            28,761
# product_review    15,892
# sales_call         6,655
            </div>

            <h3>1.2 Data Quality Assessment</h3>
            <div class="code-block">
# Assess data quality
import matplotlib.pyplot as plt
import seaborn as sns

def assess_data_quality(df):
    """
    Analyze data quality metrics
    """
    
    # Text length distribution
    df['text_length'] = df['feedback_text'].str.len()
    df['word_count'] = df['feedback_text'].str.split().str.len()
    
    print("Text Length Statistics:")
    print(df[['text_length', 'word_count']].describe())
    
    # Identify potential issues
    issues = {
        'Too Short (<10 words)': (df['word_count'] < 10).sum(),
        'Potentially Spam (>500 words)': (df['word_count'] > 500).sum(),
        'Missing Customer ID': df['customer_id'].isna().sum(),
        'Duplicate Text': df['feedback_text'].duplicated().sum()
    }
    
    print("\nData Quality Issues:")
    for issue, count in issues.items():
        print(f"  {issue}: {count:,} ({count/len(df)*100:.2f}%)")
    
    # Language detection
    from langdetect import detect
    
    df['language'] = df['feedback_text'].apply(
        lambda x: detect(x) if len(x) > 20 else 'unknown'
    )
    
    print("\nLanguage Distribution:")
    print(df['language'].value_counts())
    
    return df

feedback_df = assess_data_quality(feedback_df)

# Results:
# Average word count: 47 words
# 3.2% too short (filtered out)
# 98.7% English language
# 1.8% duplicates (removed)
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 2: Text Preprocessing & Feature Engineering</h2>

            <h3>2.1 Text Cleaning Pipeline</h3>
            <div class="code-block">
# R script for text preprocessing
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)

preprocess_text <- function(text_vector) {
  """
  Comprehensive text preprocessing pipeline
  """
  
  cleaned_text <- text_vector %>%
    # Convert to lowercase
    tolower() %>%
    
    # Remove URLs
    str_remove_all("http\\S+|www\\S+") %>%
    
    # Remove email addresses
    str_remove_all("\\S+@\\S+") %>%
    
    # Remove phone numbers
    str_remove_all("\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}") %>%
    
    # Remove special characters (keep alphanumeric and spaces)
    str_remove_all("[^a-zA-Z0-9\\s]") %>%
    
    # Remove extra whitespace
    str_squish() %>%
    
    # Remove very short words (< 3 characters)
    str_remove_all("\\b\\w{1,2}\\b")
  
  return(cleaned_text)
}

# Apply preprocessing
feedback_clean <- feedback_df %>%
  mutate(cleaned_text = preprocess_text(feedback_text))

# Example transformation:
# Before: "The API is TERRIBLE!!! It's been down 3x this week. Call me @ 555-1234"
# After: "api terrible been down this week"
            </div>

            <h3>2.2 Tokenization & Stopword Removal</h3>
            <div class="code-block">
# Tokenize and remove stopwords
library(stopwords)

# Custom stopwords for LexisNexis domain
custom_stopwords <- c(
  "lexisnexis", "lexis", "nexis",  # Company names
  "product", "service", "system",   # Generic terms
  "customer", "user", "account",    # Common nouns
  "please", "thanks", "thank",      # Pleasantries
  "need", "want", "would", "could"  # Auxiliary verbs
)

# Combine with standard stopwords
all_stopwords <- c(stopwords::stopwords("en"), custom_stopwords)

# Tokenize
feedback_tokens <- feedback_clean %>%
  unnest_tokens(word, cleaned_text) %>%
  # Remove stopwords
  anti_join(data.frame(word = all_stopwords), by = "word") %>%
  # Remove numbers
  filter(!str_detect(word, "^\\d+$"))

# Lemmatization (reduce words to base form)
feedback_tokens <- feedback_tokens %>%
  mutate(lemma = lemmatize_words(word))

print(paste("Total tokens after preprocessing:", nrow(feedback_tokens)))
print(paste("Unique tokens:", n_distinct(feedback_tokens$lemma)))

# Output:
# Total tokens: 2,847,392
# Unique tokens: 18,743
            </div>

            <h3>2.3 Bigram & Trigram Extraction</h3>
            <div class="code-block">
# Extract meaningful phrases (bigrams and trigrams)
library(tidytext)

# Bigrams
feedback_bigrams <- feedback_clean %>%
  unnest_tokens(bigram, cleaned_text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% all_stopwords,
         !word2 %in% all_stopwords) %>%
  unite(bigram, word1, word2, sep = " ")

# Count bigram frequency
bigram_counts <- feedback_bigrams %>%
  count(bigram, sort = TRUE)

# Top bigrams
print("Top 20 Bigrams:")
print(head(bigram_counts, 20))

# Results:
# "data quality" (3,421 occurrences)
# "customer support" (2,987)
# "search functionality" (2,654)
# "slow response" (2,341)
# "pricing model" (2,198)
# "user interface" (2,087)

# Trigrams for more context
feedback_trigrams <- feedback_clean %>%
  unnest_tokens(trigram, cleaned_text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% all_stopwords,
         !word2 %in% all_stopwords,
         !word3 %in% all_stopwords) %>%
  unite(trigram, word1, word2, word3, sep = " ")
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 3: Topic Modeling with LDA</h2>

            <h3>3.1 Document-Term Matrix Creation</h3>
            <div class="code-block">
# Create Document-Term Matrix for LDA
library(topicmodels)

# Reconstruct documents from tokens
feedback_dtm <- feedback_tokens %>%
  count(feedback_id, lemma) %>%
  cast_dtm(feedback_id, lemma, n)

print(feedback_dtm)

# Output:
# <<DocumentTermMatrix (documents: 87,234, terms: 18,743)>>
# Non-/sparse entries: 2,847,392/1,632,145,870
# Sparsity: 99.8%
# Maximal term length: 23

# Remove sparse terms (appear in <0.5% of documents)
feedback_dtm_trimmed <- removeSparseTerms(feedback_dtm, sparse = 0.995)
print(feedback_dtm_trimmed)

# Output after trimming:
# <<DocumentTermMatrix (documents: 87,234, terms: 2,341)>>
# Sparsity: 95.2%
            </div>

            <h3>3.2 Optimal Number of Topics (K)</h3>
            <div class="code-block">
# Determine optimal number of topics using coherence score
library(ldatuning)
library(parallel)

# Test different numbers of topics
topic_range <- seq(5, 50, by = 5)

# This takes time, so use parallel processing
cl <- makeCluster(detectCores() - 1)

# Calculate metrics for each K
result <- FindTopicsNumber(
  feedback_dtm_trimmed,
  topics = topic_range,
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 42),
  mc.cores = detectCores() - 1,
  verbose = TRUE
)

stopCluster(cl)

# Visualize results
FindTopicsNumber_plot(result)

# Analysis of results:
# - Griffiths2004: Higher is better ‚Üí peaks at K=25
# - CaoJuan2009: Lower is better ‚Üí minimizes at K=20
# - Arun2010: Lower is better ‚Üí minimizes at K=22
# - Deveaud2014: Higher is better ‚Üí peaks at K=23

# Consensus: K = 22-25 topics optimal

# Validate with coherence score
library(topicdoc)

coherence_scores <- data.frame()

for (k in 20:30) {
  lda_model <- LDA(feedback_dtm_trimmed, k = k, 
                   method = "Gibbs",
                   control = list(seed = 42, iter = 1000))
  
  coherence <- topic_coherence(lda_model, feedback_dtm_trimmed)
  
  coherence_scores <- rbind(coherence_scores, data.frame(
    k = k,
    avg_coherence = mean(coherence)
  ))
  
  print(paste("K =", k, "| Coherence:", round(mean(coherence), 4)))
}

# Best coherence at K = 24
optimal_k <- coherence_scores$k[which.max(coherence_scores$avg_coherence)]
print(paste("Optimal K:", optimal_k))
            </div>

            <h3>3.3 Final LDA Model Training</h3>
            <div class="code-block">
# Train final LDA model with K=24
set.seed(42)

lda_final <- LDA(
  feedback_dtm_trimmed,
  k = 24,
  method = "Gibbs",
  control = list(
    seed = 42,
    iter = 2000,
    burnin = 500,
    alpha = 0.1,      # Document-topic density (lower = fewer topics per doc)
    beta = 0.01,      # Topic-word density (lower = fewer words per topic)
    keep = 50,        # Save log-likelihood every 50 iterations
    verbose = 100
  )
)

# Save model
saveRDS(lda_final, "lda_model_k24.rds")

# Extract topics
lda_topics <- tidy(lda_final, matrix = "beta")  # Word-topic probabilities
lda_docs <- tidy(lda_final, matrix = "gamma")   # Document-topic probabilities

# Top terms per topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

print("Top 10 words for each topic:")
print(top_terms)
            </div>

            <h3>3.4 Topic Interpretation & Labeling</h3>
            <div class="code-block">
# Interpret and label topics based on top words
library(wordcloud)

# Generate word clouds for each topic
for (topic_num in 1:24) {
  
  topic_words <- top_terms %>%
    filter(topic == topic_num) %>%
    arrange(desc(beta))
  
  # Create word cloud
  png(paste0("topic_", topic_num, "_wordcloud.png"), 
      width = 800, height = 600)
  
  wordcloud(words = topic_words$term,
            freq = topic_words$beta,
            max.words = 50,
            colors = brewer.pal(8, "Dark2"))
  
  dev.off()
}

# Manual topic labeling based on inspection
topic_labels <- data.frame(
  topic = 1:24,
  label = c(
    "Data Quality Issues",
    "Search Functionality",
    "Customer Support Experience",
    "Pricing & Contracts",
    "User Interface & UX",
    "API Performance",
    "Data Coverage & Completeness",
    "Training & Onboarding",
    "Report Generation",
    "Integration Issues",
    "Login & Authentication",
    "Mobile App Experience",
    "Billing & Invoicing",
    "Response Time & Speed",
    "Export Functionality",
    "Compliance & Regulations",
    "Alerts & Notifications",
    "Customization Options",
    "Documentation Quality",
    "Data Accuracy",
    "Platform Stability",
    "Feature Requests",
    "Competitor Comparisons",
    "General Satisfaction"
  )
)

# Example: Topic 1 - Data Quality Issues
# Top words: quality, accuracy, outdated, incorrect, missing, error, validate

# Example: Topic 2 - Search Functionality
# Top words: search, find, filter, query, results, relevance, advanced

print("Topic Labels:")
print(topic_labels)
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 4: Model Evaluation & Validation</h2>

            <h3>4.1 Perplexity & Log-Likelihood</h3>
            <div class="code-block">
# Calculate perplexity (lower is better)
perplexity_score <- perplexity(lda_final, newdata = feedback_dtm_trimmed)
print(paste("Model Perplexity:", round(perplexity_score, 2)))

# Log-likelihood over iterations
logLiks <- lda_final@logLiks

ggplot(data.frame(iteration = 1:length(logLiks), logLik = logLiks),
       aes(x = iteration, y = logLik)) +
  geom_line(color = "#3498db", size = 1) +
  labs(
    title = "LDA Model Convergence",
    x = "Iteration",
    y = "Log-Likelihood"
  ) +
  theme_minimal()

# Model converged after ~1,200 iterations
            </div>

            <h3>4.2 Human Evaluation</h3>
            <div class="code-block">
# Sample feedback for human validation
set.seed(123)

# Get dominant topic for each document
doc_topics <- lda_docs %>%
  group_by(document) %>%
  slice_max(gamma, n = 1) %>%
  ungroup()

# Sample 10 documents per topic for human review
validation_sample <- doc_topics %>%
  group_by(topic) %>%
  slice_sample(n = 10) %>%
  left_join(feedback_clean, by = c("document" = "feedback_id")) %>%
  left_join(topic_labels, by = "topic") %>%
  select(document, topic, label, feedback_text, gamma)

# Export for manual review
write.csv(validation_sample, "topic_validation_sample.csv", row.names = FALSE)

# Human evaluators assess agreement
# Results:
# - Average agreement: 87.3%
# - High agreement (>90%): 18 topics
# - Medium agreement (70-90%): 5 topics
# - Low agreement (<70%): 1 topic (Topic 24 - General Satisfaction, too broad)

# Decision: Merge Topic 24 into other related topics
            </div>

            <h3>4.3 Topic Coherence Analysis</h3>
            <div class="code-block">
# Calculate topic coherence for interpretability
library(topicdoc)

# Coherence measures semantic similarity of top words
coherence_scores <- topic_coherence(lda_final, feedback_dtm_trimmed, 
                                   top_n_tokens = 10)

coherence_df <- data.frame(
  topic = 1:24,
  coherence = coherence_scores
) %>%
  left_join(topic_labels, by = "topic") %>%
  arrange(desc(coherence))

print("Topic Coherence Scores:")
print(coherence_df)

# Top coherent topics:
# 1. API Performance (0.482)
# 2. Login & Authentication (0.471)
# 3. Billing & Invoicing (0.458)
# 4. Search Functionality (0.442)

# Less coherent (needs refinement):
# 22. Feature Requests (0.312)
# 24. General Satisfaction (0.287)
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 5: Dashboard Development</h2>

            <h3>5.1 Power BI Dashboard</h3>
            <p>Created executive-level Power BI dashboard with:</p>
            <ul>
                <li><strong>Topic Distribution Overview:</strong> Pie chart showing % of feedback by topic</li>
                <li><strong>Trend Analysis:</strong> Line charts showing topic prevalence over time</li>
                <li><strong>Sentiment by Topic:</strong> Combined topic modeling with sentiment analysis</li>
                <li><strong>Word Clouds:</strong> Visual representation of top words per topic</li>
                <li><strong>Source Breakdown:</strong> Topics by feedback channel (survey, ticket, etc.)</li>
                <li><strong>Product Line View:</strong> Topic distribution by product/service line</li>
                <li><strong>Customer Segment Analysis:</strong> Enterprise vs. SMB topic differences</li>
            </ul>

            <div class="highlight-box">
                <strong>Key Insights from Dashboard:</strong>
                <ul style="margin-top: 0.5rem;">
                    <li>Data Quality Issues (#1) comprise 18.4% of all feedback</li>
                    <li>Search Functionality complaints increased 34% month-over-month</li>
                    <li>Enterprise customers 2.3x more likely to mention API Performance</li>
                    <li>Customer Support Experience topic has 62% positive sentiment</li>
                </ul>
            </div>

            <h3>5.2 R Shiny Interactive Application</h3>
            <div class="code-block">
# R Shiny app for deep-dive analysis
library(shiny)
library(shinydashboard)
library(DT)
library(plotly)

ui <- dashboardPage(
  dashboardHeader(title = "NLP Topic Explorer"),
  
  dashboardSidebar(
    sidebarMenu(
      menuItem("Topic Overview", tabName = "overview"),
      menuItem("Topic Deep Dive", tabName = "deep_dive"),
      menuItem("Trend Analysis", tabName = "trends"),
      menuItem("Search Feedback", tabName = "search"),
      menuItem("Comparison", tabName = "compare")
    )
  ),
  
  dashboardBody(
    tabItems(
      # Overview tab
      tabItem(tabName = "overview",
        fluidRow(
          valueBoxOutput("total_feedback"),
          valueBoxOutput("total_topics"),
          valueBoxOutput("avg_coherence")
        ),
        fluidRow(
          box(
            title = "Topic Distribution",
            plotlyOutput("topic_distribution_plot"),
            width = 6
          ),
          box(
            title = "Topic Network",
            plotOutput("topic_network_plot"),
            width = 6
          )
        ),
        fluidRow(
          box(
            title = "Recent Feedback by Topic",
            DTOutput("recent_feedback_table"),
            width = 12
          )
        )
      ),
      
      # Deep Dive tab
      tabItem(tabName = "deep_dive",
        fluidRow(
          box(
            title = "Select Topic",
            selectInput("selected_topic", "Topic:",
                       choices = topic_labels$label,
                       selected = topic_labels$label[1]),
            width = 12
          )
        ),
        fluidRow(
          box(
            title = "Top Words",
            plotOutput("wordcloud_plot"),
            width = 6
          ),
          box(
            title = "Topic Statistics",
            tableOutput("topic_stats_table"),
            width = 6
          )
        ),
        fluidRow(
          box(
            title = "Sample Feedback",
            DTOutput("sample_feedback_table"),
            width = 12
          )
        )
      ),
      
      # Trend Analysis tab
      tabItem(tabName = "trends",
        fluidRow(
          box(
            title = "Topic Trends Over Time",
            selectInput("trend_topics", "Select Topics:",
                       choices = topic_labels$label,
                       selected = topic_labels$label[1:5],
                       multiple = TRUE),
            dateRangeInput("date_range", "Date Range:",
                          start = Sys.Date() - 180,
                          end = Sys.Date()),
            width = 12
          )
        ),
        fluidRow(
          box(
            title = "Trend Chart",
            plotlyOutput("trend_chart"),
            width = 12
          )
        ),
        fluidRow(
          box(
            title = "Growth Rate Analysis",
            tableOutput("growth_rate_table"),
            width = 6
          ),
          box(
            title = "Seasonality Detection",
            plotOutput("seasonality_plot"),
            width = 6
          )
        )
      ),
      
      # Search tab
      tabItem(tabName = "search",
        fluidRow(
          box(
            title = "Search Feedback",
            textInput("search_term", "Search Term:", ""),
            actionButton("search_btn", "Search", class = "btn-primary"),
            width = 12
          )
        ),
        fluidRow(
          box(
            title = "Search Results",
            DTOutput("search_results_table"),
            width = 12
          )
        )
      )
    )
  )
)

server <- function(input, output, session) {
  
  # Load model and data
  lda_model <- readRDS("lda_model_k24.rds")
  
  # Value boxes
  output$total_feedback <- renderValueBox({
    valueBox(
      format(nrow(feedback_clean), big.mark = ","),
      "Total Feedback Items",
      icon = icon("comments"),
      color = "blue"
    )
  })
  
  output$total_topics <- renderValueBox({
    valueBox(
      24,
      "Topics Identified",
      icon = icon("tags"),
      color = "green"
    )
  })
  
  # Topic distribution plot
  output$topic_distribution_plot <- renderPlotly({
    topic_dist <- doc_topics %>%
      count(topic) %>%
      left_join(topic_labels, by = "topic") %>%
      mutate(percentage = n / sum(n) * 100)
    
    plot_ly(topic_dist, labels = ~label, values = ~percentage,
            type = "pie",
            textposition = "inside",
            textinfo = "label+percent") %>%
      layout(title = "Feedback Distribution by Topic")
  })
  
  # Word cloud for selected topic
  output$wordcloud_plot <- renderPlot({
    topic_num <- which(topic_labels$label == input$selected_topic)
    
    topic_words <- top_terms %>%
      filter(topic == topic_num) %>%
      arrange(desc(beta))
    
    wordcloud(words = topic_words$term,
              freq = topic_words$beta,
              max.words = 50,
              colors = brewer.pal(8, "Dark2"),
              random.order = FALSE)
  })
  
  # Trend analysis
  output$trend_chart <- renderPlotly({
    selected_topic_nums <- which(topic_labels$label %in% input$trend_topics)
    
    trend_data <- doc_topics %>%
      filter(topic %in% selected_topic_nums) %>%
      left_join(feedback_clean, by = c("document" = "feedback_id")) %>%
      left_join(topic_labels, by = "topic") %>%
      mutate(week = floor_date(submission_date, "week")) %>%
      count(week, label)
    
    plot_ly(trend_data, x = ~week, y = ~n, color = ~label,
            type = "scatter", mode = "lines") %>%
      layout(
        title = "Topic Trends",
        xaxis = list(title = "Week"),
        yaxis = list(title = "Number of Mentions")
      )
  })
}

shinyApp(ui, server)
            </div>
        </div>

        <div class="impact-section">
            <h2>Business Results & Impact</h2>
            
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-number">87%</div>
                    <div class="metric-label">Categorization Accuracy</div>
                </div>
                <div class="metric-card">
                    <div class="metric-number">94%</div>
                    <div class="metric-label">Time Savings (Manual Analysis)</div>
                </div>
                <div class="metric-card">
                    <div class="metric-number">12%</div>
                    <div class="metric-label">Improvement in First-Call Resolution</div>
                </div>
                <div class="metric-card">
                    <div class="metric-number">24hrs</div>
                    <div class="metric-label">Time to Insights (from 3 weeks)</div>
                </div>
            </div>

            <h3>Key Business Outcomes</h3>
            <ul>
                <li><strong>Automated Analysis:</strong> Reduced 160 hours/month of manual categorization to 10 hours of review</li>
                <li><strong>Faster Insights:</strong> Real-time topic identification enabled rapid response to emerging issues</li>
                <li><strong>Product Improvements:</strong> Data Quality topic insights led to $2.1M platform improvement initiative</li>
                <li><strong>Support Efficiency:</strong> Armed support team with knowledge of trending issues, improving FCR by 12%</li>
                <li><strong>Customer Satisfaction:</strong> Proactive addressing of "Search Functionality" complaints increased NPS by 8 points</li>
                <li><strong>Executive Visibility:</strong> Dashboard enabled C-level tracking of customer voice trends</li>
            </ul>

            <h3>Top Discovered Insights</h3>
            <ol>
                <li><strong>Data Quality Crisis:</strong> 18% of feedback related to data accuracy ‚Üí $2.1M investment in data pipeline improvements</li>
                <li><strong>Search Deterioration:</strong> 34% monthly increase in search complaints ‚Üí emergency UX redesign project initiated</li>
                <li><strong>API Complexity:</strong> Enterprise customers struggling with API integration ‚Üí created new developer resources</li>
                <li><strong>Documentation Gap:</strong> 12% of feedback requesting better docs ‚Üí hired technical writer, created video tutorials</li>
                <li><strong>Billing Confusion:</strong> Invoice topic appearing in 8% of feedback ‚Üí simplified billing statements</li>
            </ol>
        </div>

        <div class="content-section">
            <h2>Tools & Technologies</h2>
            <div class="tech-stack">
                <span class="tech-badge">R</span>
                <span class="tech-badge">Python</span>
                <span class="tech-badge">LDA (Latent Dirichlet Allocation)</span>
                <span class="tech-badge">NLP</span>
                <span class="tech-badge">topicmodels</span>
                <span class="tech-badge">tidytext</span>
                <span class="tech-badge">tm (Text Mining)</span>
                <span class="tech-badge">Power BI</span>
                <span class="tech-badge">R Shiny</span>
                <span class="tech-badge">PostgreSQL</span>
                <span class="tech-badge">ggplot2</span>
                <span class="tech-badge">plotly</span>
            </div>

            <h3>Project Timeline</h3>
            <p><strong>Total Duration:</strong> 6 months (Jan - Jun 2023)</p>
            <ul>
                <li>Data Collection & Integration: 4 weeks</li>
                <li>Text Preprocessing Pipeline: 3 weeks</li>
                <li>EDA & Topic Exploration: 2 weeks</li>
                <li>LDA Model Development & Tuning: 5 weeks</li>
                <li>Model Validation & Refinement: 3 weeks</li>
                <li>Power BI Dashboard Development: 3 weeks</li>
                <li>R Shiny App Development: 4 weeks</li>
                <li>User Testing & Iteration: 2 weeks</li>
            </ul>
        </div>

        <a href="index.html" class="back-button">‚Üê Back to Portfolio</a>
    </div>
</body>
</html>
