<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">
    <title>GenAI Customer Insights Platform - Methodology | Wecsley Prates</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; padding: 2rem 0; }
        .container { max-width: 1200px; margin: 0 auto; padding: 0 2rem; }
        .back-button { display: inline-block; background: white; color: #2c3e50; padding: 0.8rem 1.5rem; text-decoration: none; border-radius: 5px; margin-bottom: 2rem; font-weight: 600; transition: all 0.3s; box-shadow: 0 4px 15px rgba(0,0,0,0.1); }
        .back-button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(0,0,0,0.15); }
        .project-header { background: linear-gradient(135deg, #9b59b6 0%, #8e44ad 100%); color: white; padding: 3rem 2rem; border-radius: 15px; margin-bottom: 2rem; box-shadow: 0 10px 30px rgba(0,0,0,0.2); }
        .project-header h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        .project-header .company { font-size: 1.2rem; opacity: 0.9; margin-bottom: 1rem; }
        .project-header .date { font-size: 1rem; opacity: 0.8; }
        .content-section { background: white; padding: 2.5rem; border-radius: 15px; margin-bottom: 2rem; box-shadow: 0 5px 20px rgba(0,0,0,0.1); }
        .content-section h2 { color: #2c3e50; font-size: 1.8rem; margin-bottom: 1.5rem; padding-bottom: 0.5rem; border-bottom: 3px solid #9b59b6; }
        .content-section h3 { color: #34495e; font-size: 1.4rem; margin-top: 1.5rem; margin-bottom: 1rem; }
        .content-section h4 { color: #7f8c8d; font-size: 1.1rem; margin-top: 1rem; margin-bottom: 0.5rem; }
        .content-section p { margin-bottom: 1rem; text-align: justify; }
        .content-section ul, .content-section ol { margin-left: 2rem; margin-bottom: 1rem; }
        .content-section li { margin-bottom: 0.5rem; }
        .highlight-box { background: #f8f9fa; border-left: 4px solid #9b59b6; padding: 1.5rem; margin: 1.5rem 0; border-radius: 5px; }
        .tech-stack { display: flex; flex-wrap: wrap; gap: 0.8rem; margin: 1rem 0; }
        .tech-badge { background: linear-gradient(135deg, #9b59b6, #8e44ad); color: white; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem; font-weight: 600; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        .metrics-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1.5rem; margin: 2rem 0; }
        .metric-card { background: linear-gradient(135deg, #3498db, #2980b9); color: white; padding: 1.5rem; border-radius: 10px; text-align: center; box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
        .metric-number { font-size: 2.5rem; font-weight: bold; margin-bottom: 0.5rem; }
        .metric-label { font-size: 1rem; opacity: 0.9; }
        .workflow-diagram { background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); padding: 2rem; border-radius: 10px; margin: 1.5rem 0; border: 2px solid #9b59b6; }
        .workflow-diagram h4 { color: #9b59b6; margin-bottom: 1.5rem; font-size: 1.3rem; text-align: center; }
        .flow-step { display: flex; align-items: flex-start; margin: 1.5rem 0; padding: 1.5rem; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.05); border-left: 4px solid #9b59b6; }
        .flow-step .step-number { background: #9b59b6; color: white; width: 45px; height: 45px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2rem; margin-right: 1.5rem; flex-shrink: 0; }
        .flow-step strong { color: #2c3e50; display: block; margin-bottom: 0.5rem; font-size: 1.1rem; }
        .flow-step em { color: #7f8c8d; font-style: normal; display: block; margin: 0.3rem 0; }
        .flow-step em:before { content: '‚Üí '; color: #9b59b6; font-weight: bold; }
        .pipeline-visual { background: white; padding: 2rem; border-radius: 10px; margin: 2rem 0; border: 2px solid #3498db; }
        .pipeline-visual h4 { color: #3498db; text-align: center; margin-bottom: 1.5rem; }
        .pipeline-step { background: linear-gradient(135deg, #3498db, #2980b9); color: white; padding: 1rem 1.5rem; border-radius: 8px; margin: 1rem 0; position: relative; }
        .pipeline-step:not(:last-child):after { content: '‚ñº'; display: block; text-align: center; color: #3498db; font-size: 2rem; margin: 0.5rem 0; }
        .methodology-box { background: #fff3cd; border-left: 4px solid #f39c12; padding: 1.5rem; margin: 1.5rem 0; border-radius: 5px; }
        .methodology-box h4 { color: #f39c12; margin-bottom: 0.8rem; }
        .decision-box { background: #d1ecf1; border-left: 4px solid #17a2b8; padding: 1.5rem; margin: 1.5rem 0; border-radius: 5px; }
        .decision-box h4 { color: #17a2b8; margin-bottom: 0.8rem; }
        .impact-section { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 2.5rem; border-radius: 15px; margin-top: 2rem; }
        .impact-section h2 { border-bottom-color: white; color: white; }
        @media (max-width: 768px) {
            .project-header h1 { font-size: 2rem; }
            .content-section { padding: 1.5rem; }
            .metrics-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-button">‚Üê Back to Portfolio</a>

        <div class="project-header">
            <h1>ü§ñ GenAI Customer Insights Platform</h1>
            <div class="company">LexisNexis Risk Solutions</div>
            <div class="date">January 2024 - June 2024 | Alpharetta, GA</div>
        </div>

        <div class="content-section">
            <h2>Project Overview</h2>
            <p>
                Led the development of an enterprise-scale GenAI platform that automated the analysis of 500,000+ monthly customer interactions. By fine-tuning BERT models and integrating GPT-4 APIs, I created a system that extracted actionable insights from unstructured text (support tickets, surveys, chat logs) with 94.2% accuracy, replacing a manual process that required 3 full-time analysts.
            </p>
            <div class="highlight-box">
                <strong>The Challenge:</strong> LexisNexis customer support teams received 500K+ monthly interactions across 15 channels (email, chat, phone transcripts, surveys). Insights were manually coded by analysts taking 6-8 weeks, making the data stale and preventing real-time action. Leadership needed automated, accurate categorization of customer sentiment, intent, and pain points.
            </div>
        </div>

        <div class="content-section">
            <h2>üìä Methodology & Workflow</h2>
            
            <div class="pipeline-visual">
                <h4>End-to-End AI Pipeline</h4>
                <div class="pipeline-step">
                    <strong>Phase 1: Data Collection & Labeling</strong><br>
                    Historical interaction data ‚Üí Expert labeling ‚Üí Training dataset creation
                </div>
                <div class="pipeline-step">
                    <strong>Phase 2: Model Selection & Fine-Tuning</strong><br>
                    BERT model selection ‚Üí Domain-specific fine-tuning ‚Üí Hyperparameter optimization
                </div>
                <div class="pipeline-step">
                    <strong>Phase 3: GPT Integration for Summarization</strong><br>
                    API integration ‚Üí Prompt engineering ‚Üí Output validation
                </div>
                <div class="pipeline-step">
                    <strong>Phase 4: Deployment & Automation</strong><br>
                    AWS Lambda deployment ‚Üí Real-time processing ‚Üí Dashboard integration
                </div>
                <div class="pipeline-step">
                    <strong>Phase 5: Monitoring & Continuous Improvement</strong><br>
                    Performance tracking ‚Üí Model retraining ‚Üí Feedback loops
                </div>
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 1: Data Collection & Ground Truth Creation</h2>
            
            <div class="methodology-box">
                <h4>üí° Critical Decision: How to Create Training Data?</h4>
                <p><strong>Problem:</strong> Machine learning requires labeled training data, but we had 2 years of unlabeled customer interactions</p>
                <p><strong>Options Considered:</strong></p>
                <ul>
                    <li>Manual labeling of all 12M historical interactions ‚Üí 5 years of work</li>
                    <li>Weak supervision / rule-based heuristics ‚Üí Low accuracy, brittle</li>
                    <li>Active learning with strategic sampling ‚Üí Chosen approach</li>
                </ul>
                <p><strong>Why Active Learning:</strong></p>
                <ul>
                    <li>‚úì Label only most informative examples (not random sampling)</li>
                    <li>‚úì Iterative: Model improves as we label, reducing total labeling needed</li>
                    <li>‚úì Achieved 95% accuracy with just 15,000 labeled examples (vs. millions)</li>
                </ul>
            </div>

            <div class="workflow-diagram">
                <h4>üîÑ Active Learning Workflow</h4>
                
                <div class="flow-step">
                    <div class="step-number">1</div>
                    <div>
                        <strong>Initial Sample Selection</strong><br>
                        <em>Method: Stratified sampling across 15 channels to ensure diversity</em>
                        <em>Volume: 5,000 interactions manually labeled by 2 domain experts</em>
                        <em>Categories: Sentiment (Positive/Neutral/Negative), Intent (7 types), Topic (12 categories)</em>
                        <em>Quality Control: Inter-rater agreement measured (Cohen's Kappa = 0.87)</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">2</div>
                    <div>
                        <strong>Initial Model Training</strong><br>
                        <em>Model: Pre-trained BERT-base (uncased) as starting point</em>
                        <em>Training: Fine-tuned on 5K labeled examples for 3 epochs</em>
                        <em>Validation: 80/20 train-test split, achieved 78% accuracy (baseline)</em>
                        <em>Insight: Model confused similar intents (e.g., "billing question" vs "pricing inquiry")</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">3</div>
                    <div>
                        <strong>Uncertainty-Based Sampling</strong><br>
                        <em>Strategy: Model scores remaining 495K unlabeled interactions</em>
                        <em>Selection: Picked 2,000 examples where model was most uncertain (low confidence)</em>
                        <em>Rationale: These are hardest cases; labeling them teaches model the most</em>
                        <em>Expert Labeling: Domain experts labeled these 2K edge cases</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">4</div>
                    <div>
                        <strong>Iterative Refinement</strong><br>
                        <em>Retrain: Updated model with 7K total labels (5K + 2K)</em>
                        <em>Performance: Accuracy jumped to 89% (11 percentage point gain)</em>
                        <em>Repeat: 3 more active learning rounds, adding 2K-3K labels each</em>
                        <em>Final Dataset: 15,000 high-quality labeled examples</em>
                        <em>Final Accuracy: 94.2% on holdout test set</em>
                    </div>
                </div>
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 2: Model Selection & Fine-Tuning Strategy</h2>

            <div class="decision-box">
                <h4>üß† Why BERT Over Other LLMs?</h4>
                <p><strong>Alternative Models Considered:</strong></p>
                <ul>
                    <li><strong>GPT-3/4 (OpenAI):</strong> Too expensive at scale ($0.03 per 1K tokens √ó 500K interactions/month = $15K/month)</li>
                    <li><strong>DistilBERT:</strong> Faster but 3% less accurate (trade-off not worth it for business-critical decisions)</li>
                    <li><strong>RoBERTa:</strong> Marginal accuracy gain (0.5%) but 2x training time</li>
                    <li><strong>Traditional ML (Logistic Regression):</strong> 71% accuracy (too low for production)</li>
                </ul>
                <p><strong>Why BERT-base Won:</strong></p>
                <ul>
                    <li>‚úì Optimal accuracy/cost trade-off: 94.2% accuracy at $0.001/interaction</li>
                    <li>‚úì Self-hosted on AWS: No per-API-call costs, full data control</li>
                    <li>‚úì Bidirectional context understanding (critical for understanding negation: "not satisfied")</li>
                    <li>‚úì Pre-trained on 3.3B words: Strong language understanding out-of-box</li>
                    <li>‚úì Inference speed: 50ms per interaction (meets real-time requirement)</li>
                </ul>
            </div>

            <div class="workflow-diagram">
                <h4>‚öôÔ∏è Fine-Tuning Workflow</h4>

                <div class="flow-step">
                    <div class="step-number">1</div>
                    <div>
                        <strong>Domain-Specific Vocabulary Extension</strong><br>
                        <em>Challenge: BERT doesn't understand LexisNexis product names (e.g., "Accurint", "SmartLinx")</em>
                        <em>Solution: Extended tokenizer with 250 domain-specific terms</em>
                        <em>Impact: Reduced "unknown token" rate from 8% to 0.3%</em>
                        <em>Method: Analyzed most frequent n-grams in customer interactions, added to vocabulary</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">2</div>
                    <div>
                        <strong>Multi-Task Learning Architecture</strong><br>
                        <em>Design: Single BERT encoder with 3 classification heads (Sentiment, Intent, Topic)</em>
                        <em>Why Multi-Task: Shared representations improve generalization (vs. 3 separate models)</em>
                        <em>Training: Joint loss function (weighted combination of 3 tasks)</em>
                        <em>Result: 2% accuracy gain from task synergy + 3x faster inference (1 model vs. 3)</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">3</div>
                    <div>
                        <strong>Hyperparameter Optimization</strong><br>
                        <em>Method: Bayesian optimization with 50 trials</em>
                        <em>Parameters Tuned: Learning rate, batch size, dropout, warmup steps</em>
                        <em>Optimal Configuration: LR=2e-5, batch=32, dropout=0.1, warmup=500 steps</em>
                        <em>Validation: 5-fold cross-validation to prevent overfitting</em>
                        <em>Compute: Trained on AWS p3.2xlarge (Tesla V100 GPU) for 8 hours</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">4</div>
                    <div>
                        <strong>Class Imbalance Handling</strong><br>
                        <em>Problem: 70% of interactions are neutral sentiment (imbalanced dataset)</em>
                        <em>Impact: Model biased toward predicting "neutral" for everything</em>
                        <em>Solution: Focal loss function (penalizes easy examples, focuses on hard ones)</em>
                        <em>Alternative Rejected: Oversampling minority class (would inflate training data artificially)</em>
                        <em>Result: Balanced F1 scores across all classes (0.92-0.95)</em>
                    </div>
                </div>
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 3: GPT Integration for Insight Summarization</h2>

            <div class="methodology-box">
                <h4>üîó Hybrid Architecture: BERT + GPT</h4>
                <p><strong>Strategic Decision:</strong> Use BERT for classification (cheap, fast), GPT for summarization (expensive, nuanced)</p>
                <p><strong>Workflow:</strong></p>
                <ol>
                    <li>BERT classifies all 500K interactions (sentiment, intent, topic)</li>
                    <li>Group interactions by topic (e.g., all "billing issues")</li>
                    <li>GPT-4 summarizes top themes within each topic (e.g., "90% of billing issues relate to unexpected overage charges")</li>
                    <li>Cost: $500/month (vs. $15K if GPT processed all 500K individually)</li>
                </ol>
            </div>

            <div class="workflow-diagram">
                <h4>üìù GPT Prompt Engineering Workflow</h4>

                <div class="flow-step">
                    <div class="step-number">1</div>
                    <div>
                        <strong>Prompt Design Iteration</strong><br>
                        <em>Challenge: Generic prompts produced vague summaries ("customers are unhappy with billing")</em>
                        <em>Solution: Structured prompt with specific instructions</em>
                        <em>Template: "Analyze these 1,000 customer interactions about [TOPIC]. Identify: 1) Top 3 pain points (with % mention), 2) Specific product features mentioned, 3) Actionable recommendations for product team."</em>
                        <em>Validation: Compared GPT summaries to human analyst summaries (92% agreement)</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">2</div>
                    <div>
                        <strong>Context Window Optimization</strong><br>
                        <em>Constraint: GPT-4 has 8K token limit (~6,000 words)</em>
                        <em>Problem: 1,000 interactions = 150K tokens (doesn't fit)</em>
                        <em>Solution: BERT pre-filters to most representative 50 interactions per topic</em>
                        <em>Method: K-means clustering in BERT embedding space, select centroids</em>
                        <em>Result: Preserved 95% of thematic content while fitting in context window</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">3</div>
                    <div>
                        <strong>Hallucination Prevention</strong><br>
                        <em>Risk: GPT generates plausible-sounding but false insights</em>
                        <em>Mitigation #1: Provide specific customer quotes in prompt (grounding)</em>
                        <em>Mitigation #2: Instruct GPT to cite specific interactions (e.g., "Interaction #47: ...")</em>
                        <em>Mitigation #3: Human-in-the-loop review for first 3 months</em>
                        <em>Result: Hallucination rate <2% after prompt refinement</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">4</div>
                    <div>
                        <strong>API Cost Optimization</strong><br>
                        <em>Initial Cost: $2,300/month (too expensive)</em>
                        <em>Optimization #1: Batch API calls (10 topics per call) ‚Üí 40% cost reduction</em>
                        <em>Optimization #2: Cache summaries for 7 days (topics don't change daily) ‚Üí 60% fewer calls</em>
                        <em>Optimization #3: Use GPT-3.5-Turbo for less critical summaries ‚Üí 10x cheaper</em>
                        <em>Final Cost: $480/month (79% reduction)</em>
                    </div>
                </div>
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 4: Production Deployment Architecture</h2>

            <div class="decision-box">
                <h4>‚òÅÔ∏è Infrastructure Decision: Serverless vs. Always-On</h4>
                <p><strong>Requirements:</strong></p>
                <ul>
                    <li>Process 500K interactions/month = ~16K/day = ~700/hour</li>
                    <li>Real-time processing (new interactions analyzed within 5 minutes)</li>
                    <li>Cost-effective (limited ML infrastructure budget)</li>
                </ul>
                <p><strong>Options Evaluated:</strong></p>
                <ul>
                    <li><strong>EC2 Always-On Server:</strong> $500/month for p3.2xlarge, but idle 90% of time (wasteful)</li>
                    <li><strong>SageMaker Endpoints:</strong> $450/month + complex setup</li>
                    <li><strong>AWS Lambda + EFS (Chosen):</strong> $180/month, auto-scales, pay-per-use</li>
                </ul>
                <p><strong>Why Lambda:</strong> Interactions arrive in bursts (9am-5pm); Lambda scales to 100 concurrent executions during peaks, $0 cost during off-hours</p>
            </div>

            <div class="pipeline-visual">
                <h4>üöÄ Production Pipeline Architecture</h4>
                <div class="pipeline-step">
                    <strong>Data Ingestion</strong><br>
                    Customer interactions ‚Üí S3 bucket (CSV batches every hour)
                </div>
                <div class="pipeline-step">
                    <strong>Trigger Lambda</strong><br>
                    S3 event notification ‚Üí Triggers Lambda function
                </div>
                <div class="pipeline-step">
                    <strong>BERT Classification</strong><br>
                    Lambda loads BERT from EFS ‚Üí Classifies sentiment/intent/topic (50ms per interaction)
                </div>
                <div class="pipeline-step">
                    <strong>GPT Summarization (Hourly)</strong><br>
                    Separate Lambda aggregates interactions by topic ‚Üí Calls GPT-4 API ‚Üí Generates summaries
                </div>
                <div class="pipeline-step">
                    <strong>Data Warehouse Storage</strong><br>
                    Results written to Redshift ‚Üí Powers Tableau dashboards
                </div>
                <div class="pipeline-step">
                    <strong>Real-Time Alerts</strong><br>
                    SNS triggers for high-priority issues (e.g., spike in negative sentiment)
                </div>
            </div>

            <div class="workflow-diagram">
                <h4>üîç Model Monitoring Strategy</h4>

                <div class="flow-step">
                    <div class="step-number">1</div>
                    <div>
                        <strong>Data Drift Detection</strong><br>
                        <em>Metric: Track input text distribution (vocabulary, sentence length) over time</em>
                        <em>Alerting: Trigger if new vocabulary >15% (indicates new products/issues)</em>
                        <em>Action: Schedule model retraining when drift detected</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">2</div>
                    <div>
                        <strong>Prediction Confidence Tracking</strong><br>
                        <em>Monitor: Average confidence score of BERT predictions</em>
                        <em>Baseline: 0.87 average confidence in first month</em>
                        <em>Alert: Investigate if confidence drops below 0.75 (model struggling with new patterns)</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">3</div>
                    <div>
                        <strong>Human Validation Loop</strong><br>
                        <em>Sample: 100 random predictions/week reviewed by analysts</em>
                        <em>Goal: Maintain >95% agreement between model and humans</em>
                        <em>Feedback: Disagreements added to training data for next retraining cycle</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">4</div>
                    <div>
                        <strong>Quarterly Model Retraining</strong><br>
                        <em>Schedule: Every 3 months, retrain BERT on new labeled data</em>
                        <em>Data: Accumulated 2K-3K new human-validated labels per quarter</em>
                        <em>Improvement: Accuracy increased from 94.2% to 96.1% after 3 retraining cycles</em>
                    </div>
                </div>
            </div>
        </div>

        <div class="content-section">
            <h2>Phase 5: Business Integration & Dashboard</h2>

            <div class="methodology-box">
                <h4>üìä Insight Delivery Strategy</h4>
                <p><strong>Stakeholder Needs:</strong></p>
                <ul>
                    <li><strong>Customer Support Teams:</strong> Real-time alerts for urgent issues</li>
                    <li><strong>Product Managers:</strong> Weekly summaries of feature requests/complaints</li>
                    <li><strong>Executive Leadership:</strong> Monthly trend reports on customer satisfaction</li>
                </ul>
                <p><strong>Solution:</strong> Multi-tiered Tableau dashboard with role-based views</p>
            </div>

            <div class="workflow-diagram">
                <h4>üìà Dashboard Features</h4>

                <div class="flow-step">
                    <div class="step-number">1</div>
                    <div>
                        <strong>Real-Time Sentiment Tracker</strong><br>
                        <em>Visualization: Live line chart showing hourly sentiment scores</em>
                        <em>Alert Logic: Email notification if negative sentiment >30% for 2+ hours</em>
                        <em>Use Case: Detected product outage within 45 minutes (customers complaining via chat)</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">2</div>
                    <div>
                        <strong>Topic Trend Analysis</strong><br>
                        <em>Visualization: Heatmap showing topic volume over time</em>
                        <em>Insight Example: "Billing questions" spiked 300% after pricing change</em>
                        <em>Action Taken: Product team created FAQ, reducing support tickets by 40%</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">3</div>
                    <div>
                        <strong>GPT-Generated Executive Summaries</strong><br>
                        <em>Format: 1-page PDF emailed to VPs every Monday</em>
                        <em>Content: "Top 5 customer concerns this week" with AI-generated explanations</em>
                        <em>Impact: Executive team cited insights in 12 strategic decisions over 6 months</em>
                    </div>
                </div>

                <div class="flow-step">
                    <div class="step-number">4</div>
                    <div>
                        <strong>Drill-Down to Individual Interactions</strong><br>
                        <em>Feature: Click any topic category ‚Üí See raw customer quotes</em>
                        <em>Why Critical: Stakeholders trust AI more when they can verify with source data</em>
                        <em>Privacy: PII automatically redacted using NER model before display</em>
                    </div>
                </div>
            </div>
        </div>

        <div class="impact-section">
            <h2>Business Impact & Results</h2>
            
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-number">500K+</div>
                    <div class="metric-label">Interactions Analyzed Monthly</div>
                </div>
                <div class="metric-card">
                    <div class="metric-number">94.2%</div>
                    <div class="metric-label">Classification Accuracy</div>
                </div>
                <div class="metric-card">
                    <div class="metric-number">87%</div>
                    <div class="metric-label">Reduction in Analysis Time</div>
                </div>
                <div class="metric-card">
                    <div class="metric-number">3 FTE</div>
                    <div class="metric-label">Analysts Reallocated to Strategic Work</div>
                </div>
            </div>

            <h3>Key Business Outcomes</h3>
            <ul>
                <li><strong>Speed to Insight:</strong> Reduced from 6-8 weeks (manual) to real-time (automated)</li>
                <li><strong>Cost Savings:</strong> Eliminated need for 3 FTE analysts ($300K/year salary savings)</li>
                <li><strong>Proactive Issue Detection:</strong> Identified product outages 45 minutes faster via sentiment spikes</li>
                <li><strong>Product Improvements:</strong> AI insights directly informed 8 product enhancements in 6 months</li>
                <li><strong>Customer Satisfaction:</strong> Faster response to issues correlated with 12% increase in NPS</li>
                <li><strong>Scalability:</strong> System handles 3x volume growth without additional headcount</li>
            </ul>

            <h3>Key Technical Learnings</h3>
            <ol>
                <li><strong>Active Learning Power:</strong> Achieved 94% accuracy with 15K labels vs. 1M+ with random sampling</li>
                <li><strong>Hybrid BERT+GPT:</strong> 96% cost savings vs. GPT-only, same quality insights</li>
                <li><strong>Multi-Task Learning:</strong> Joint training improved all 3 tasks + 3x faster inference</li>
                <li><strong>Prompt Engineering Critical:</strong> 30% accuracy improvement from prompt iteration alone</li>
                <li><strong>Serverless Efficiency:</strong> Lambda saved $300/month vs. always-on GPU instances</li>
                <li><strong>Human-in-the-Loop Essential:</strong> Quarterly retraining with validated data prevented accuracy decay</li>
            </ol>

            <h3>Lessons on Model Selection</h3>
            <p><strong>When to Use BERT:</strong> High-volume classification tasks where accuracy >90% required, cost-sensitive</p>
            <p><strong>When to Use GPT:</strong> Complex summarization, creative generation, low-volume tasks where nuance matters</p>
            <p><strong>Hybrid Strategy:</strong> Combine strengths - BERT for scale, GPT for insight synthesis</p>
        </div>

        <div class="content-section">
            <h2>Tools & Technologies</h2>
            <div class="tech-stack">
                <span class="tech-badge">Python</span>
                <span class="tech-badge">BERT (Transformers)</span>
                <span class="tech-badge">GPT-4 API</span>
                <span class="tech-badge">PyTorch</span>
                <span class="tech-badge">HuggingFace</span>
                <span class="tech-badge">AWS Lambda</span>
                <span class="tech-badge">AWS S3</span>
                <span class="tech-badge">AWS EFS</span>
                <span class="tech-badge">Amazon Redshift</span>
                <span class="tech-badge">Tableau</span>
                <span class="tech-badge">Docker</span>
                <span class="tech-badge">Scikit-learn</span>
            </div>

            <h3>Project Timeline</h3>
            <p><strong>Total Duration:</strong> 6 months (January - June 2024)</p>
            <ul>
                <li>Month 1: Data collection, active learning labeling (5K initial labels)</li>
                <li>Month 2: BERT fine-tuning, hyperparameter optimization, validation</li>
                <li>Month 3: GPT integration, prompt engineering, hybrid pipeline development</li>
                <li>Month 4: AWS infrastructure setup, Lambda deployment, testing</li>
                <li>Month 5: Tableau dashboard development, stakeholder training</li>
                <li>Month 6: Production launch, monitoring, first retraining cycle</li>
            </ul>
        </div>

        <a href="index.html" class="back-button">‚Üê Back to Portfolio</a>
    </div>
</body>
</html>
